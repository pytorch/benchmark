from pathlib import Path
import re
import functools

def is_userbenchmark_runscript(run_script_file):
    MAGIC_LINE = "# GENERATED BY userbenchmark/release-test/__init__.py. DO NOT EDIT!"
    with open(run_script_file, "r") as rsf:
        script = rsf.read()
    if MAGIC_LINE in script:
        return True
    return False

def get_run_keys(work_dir: Path):
    run_keys = []
    for subdir in filter(lambda x: x.is_dir(), work_dir.iterdir()):
        run_script_file = subdir.joinpath("run.sh")
        if run_script_file.is_file() and is_userbenchmark_runscript(run_script_file):
            run_keys.append(subdir.name)
    return run_keys

def get_workloads(run_dir: Path):
    return list(map(lambda x: x.name, filter(lambda x: x.is_dir(), run_dir.iterdir())))

def dump_result_csv(work_dir, result):
    csv_object = [["Benchmark"]]
    DELIMITER = ";"
    # generate header
    run_keys = sorted(result.keys())
    workloads = sorted(result[run_keys[0]])
    metrics = sorted(result[run_keys[0]][workloads[0]])
    for run_key in run_keys:
        csv_object[0].append(f"{run_key}")
    # generate data
    for run_key in run_keys:
        for wl_id, workload in enumerate(workloads):
            for mid, metric in enumerate(metrics):
                if len(csv_object) <= len(workloads) * len(metrics):
                    csv_object.append([f"{workload}-{metric}"])
                csv_object[wl_id*len(metrics)+mid+1].append(str(result[run_key][workload][metric]))
    csv_text = []
    for csv_line in csv_object:
        csv_text.append(DELIMITER.join(csv_line))
    csv_text = "\n".join(csv_text) + "\n"
    print(csv_text)
    summary_file = work_dir.joinpath("summary.csv")
    # write result file to summary
    with open(summary_file, "w") as sf:
        sf.write(csv_text)


def get_peak_mem(mem_log):
    # example log:
    # Max GPU Mem.   Max RSS Mem.   Max PSS Mem.
    # 697            1971.07        1438.21
    max_gpu_mem = 0.0
    max_cpu_mem = 0.0
    for line in mem_log:
        numbers = re.split('\s+', line.strip())
        if len(numbers) == 3:
            gpu_mem = float(numbers[0])
            cpu_mem = float(numbers[1])
            max_gpu_mem = gpu_mem if gpu_mem > max_gpu_mem else max_gpu_mem
            max_cpu_mem = cpu_mem if cpu_mem > max_cpu_mem else max_cpu_mem
    return max_gpu_mem, max_cpu_mem

def analyze_workload(run_dir: Path, workload_name: str, res):
    workload_dir = run_dir.joinpath(workload_name)
    assert workload_dir.joinpath("result.log").exists() and workload_dir.joinpath("result_mem.log").exists(), \
        f"Error: missing benchmark result file result.log or result_mem.log in {workload_dir}."
    LATENCY_REGEX = "Total time elapsed: (.*) seconds."
    with open(workload_dir.joinpath("result.log"), "r") as lf:
        latency_log = lf.readlines()[-1].strip()
    with open(workload_dir.joinpath("result_mem.log"), "r") as mf:
        mem_log = mf.readlines()
    latency = re.search(LATENCY_REGEX, latency_log).groups()[0]
    res[workload_name] = {}
    res[workload_name]["latency"] = latency
    res[workload_name]["gpu_memory"], res[workload_name]["cpu_memory"]  = get_peak_mem(mem_log)
    return res

def dump_userbenchmark_result(results):
    metrics = {}
    for run_key in results:
        for workload in results[run_key]:
            for metric in results[run_key][workload]:
                metric_name = f"{run_key}-{workload}-{metric}"
                metrics[metric_name] = results[run_key][workload][metric]
    return metrics

def analyze_run_key(work_dir, run_key, r):
    run_dir = work_dir.joinpath(run_key)
    workloads = get_workloads(run_dir)
    workload_results = functools.reduce(lambda r, w: analyze_workload(run_dir, w, r), workloads, {})
    r[run_key] = workload_results
    return r

def analyze(work_dir: Path):
    # get base_args (directory starting with "pytorch-")
    work_dir = Path(work_dir)
    run_keys = get_run_keys(work_dir)
    assert run_keys, f"Expected non-empty run keys, get {run_keys}"
    results = functools.reduce(lambda r, k: analyze_run_key(work_dir, k, r), run_keys, {})
    # dump result to csv file
    dump_result_csv(work_dir, results)
    # dump results to userbenchmark object
    results = dump_userbenchmark_result(results)
    return results