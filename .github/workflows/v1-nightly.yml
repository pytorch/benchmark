name: TorchBench V1 nightly
on:
  workflow_dispatch:
  schedule:
    - cron: '0 14 * * *' # run at 2 PM UTC

jobs:
  run-benchmark:
    env:
      # Set to "v1-beta" for testing. Will set it to "v1" in release.
      TORCHBENCH_VER: "v1-beta"
      CONFIG_VER: "v1"
      CONDA_ENV_NAME:  "torchbench-v1-nightly-ci"
      OUTPUT_DIR: ".torchbench/v1-nightly-ci"
      CUDA_VERSION: "cu102"
      SCRIBE_GRAPHQL_ACCESS_TOKEN: ${{ secrets.SCRIBE_GRAPHQL_ACCESS_TOKEN }}
      AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}
      AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
    if: ${{ github.repository_owner == 'pytorch' }}
    runs-on: [self-hosted, bm-runner]
    steps:
      - name: Checkout
        uses: actions/checkout@v2
        with:
          ref: v1.0
      - name: Create conda env
        run: |
          conda create -y -q --name "${CONDA_ENV_NAME}" python=3.7
      - name: Install PyTorch nightly
        run: |
          . activate "${CONDA_ENV_NAME}"
          # Install dependencies
          pip install requests bs4 argparse
          # Check if nightly builds are available
          NIGHTLIES=$(python torchbenchmark/util/torch_nightly.py --packages torch)
          # If failed, the script will generate empty result
          if [ -z $NIGHTLIES ]; then
              echo "Torch nightly build failed. Cancel the workflow."
              exit 1
          fi
          # Install PyTorch nightly from pip
          pip install --pre torch torchtext torchvision \
          -f https://download.pytorch.org/whl/nightly/${CUDA_VERSION}/torch_nightly.html
      - name: Install other TorchBench dependencies
        run: |
          . activate "${CONDA_ENV_NAME}"
          python install.py
      - name: Run benchmark
        run: |
          . activate "${CONDA_ENV_NAME}"
          bash ./.github/scripts/run.sh "${HOME}/${OUTPUT_DIR}/gh${GITHUB_RUN_ID}"
      - name: Generate the bisection config
        run: |
          . activate "${CONDA_ENV_NAME}"
          python ./.github/scripts/generate-abtest-config.py \
                 --benchmark-dir "${HOME}/${OUTPUT_DIR}/gh${GITHUB_RUN_ID}" \
                 --out "${HOME}/${OUTPUT_DIR}/gh${GITHUB_RUN_ID}/bisection.yaml"
      - name: Copy artifact and upload to scribe
        run: |
          . activate "${CONDA_ENV_NAME}"
          LATEST_RESULT=$(find ${HOME}/${OUTPUT_DIR}/gh${GITHUB_RUN_ID} -name "*.json" | sort -r | head -1)
          echo "Benchmark result file: $LATEST_RESULT"
          TODAY=$(date "+%Y%m%d%H%M%S")
          CONFIG_DIR=torchbenchmark/score/configs/${CONFIG_VER}
          CONFIG_ENV=${CONFIG_DIR}/config-${CONFIG_VER}.env
          # Load environment variables
          set -a; source "${CONFIG_ENV}"; set +a
          CONFIG_NORM_FILE="${CONFIG_DIR}/${CONFIG_FILE}"
          SCORE_FILE="./benchmark-result-v1-score-${TODAY}.json"
          # Generate score file
          python compute_score.py --score_version v1 --benchmark_data_file "${LATEST_RESULT}" > "${SCORE_FILE}"
          # Upload result to Scribe
          python scripts/upload_scribe.py --pytest_bench_json "${LATEST_RESULT}" --torchbench_score_file "${SCORE_FILE}"
          mkdir -p benchmark-output
          cp "${LATEST_RESULT}" ./benchmark-output/benchmark-result-v1-${TODAY}.json
          cp "${HOME}/${OUTPUT_DIR}/gh${GITHUB_RUN_ID}/bisection.yaml" ./benchmark-output/bisection-v1-${TODAY}.yaml
      - name: Upload artifact
        uses: actions/upload-artifact@v2
        with:
          name: Benchmark result
          path: benchmark-output/
      - name: Destroy conda env
        run: |
          conda env remove --name "${CONDA_ENV_NAME}"
