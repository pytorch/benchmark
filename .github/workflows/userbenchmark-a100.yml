name: TorchBench Userbenchmarks on A100
on:
  schedule:
    - cron: '0 14 * * *' # run at 2 PM UTC
  workflow_dispatch:
    inputs:
      userbenchmark_name:
        description: "Name of the user benchmark to run"
        required: true
env:
  PYTHON_VERSION: "3.8"
  TENSORRT_PYTHON_VERSION: "cp38"
  TENSORRT_VERSION: "8.2.4.2"
  CUDA_VERSION: "cu113"
  CONDA_ENV_NAME: "userbenchmarks-ci"
  MAGMA_VERSION: "magma-cuda113"
  TORCHBENCH_USERBENCHMARK_SCRIBE_GRAPHQL_ACCESS_TOKEN: ${{ secrets.TORCHBENCH_USERBENCHMARK_SCRIBE_GRAPHQL_ACCESS_TOKEN }}
  SETUP_SCRIPT: "/data/shared/setup_instance.sh"
jobs:
  run-userbenchmark:
    runs-on: [self-hosted, a100-runner]
    timeout-minutes: 1440 # 24 hours
    steps:
      - name: Checkout TorchBench
        uses: actions/checkout@v2
        with:
          path: benchmark
      - name: Create conda environment
        run: |
          conda create -y -q --name "${CONDA_ENV_NAME}" python="${PYTHON_VERSION}"
      - name: Install PyTorch nightly
        run: |
          . "${SETUP_SCRIPT}" && conda activate "${CONDA_ENV_NAME}"
          pushd benchmark
          # Install dependencies
          conda install -y -c pytorch "${MAGMA_VERSION}"
          pip install requests bs4 argparse gitpython boto3
          # Check if nightly builds are available
          NIGHTLIES=$(python torchbenchmark/util/torch_nightly.py --packages torch)
          # If failed, the script will generate empty result
          if [ -z $NIGHTLIES ]; then
              echo "Torch nightly build failed. Cancel the workflow."
              exit 1
          fi
          # Install PyTorch nightly from pip
          pip install --pre torch torchtext torchvision \
            -f https://download.pytorch.org/whl/nightly/${CUDA_VERSION}/torch_nightly.html
          # make sure pytorch+cuda works
          python -c "import torch; torch.cuda.init()"
      - name: Run user benchmark
        run: |
          . "${SETUP_SCRIPT}" && conda activate "${CONDA_ENV_NAME}"
          # remove old results
          if [ -d benchmark-output ]; then rm -Rf benchmark-output; fi
          pushd benchmark
          if [ -d .userbenchmark ]; then rm -Rf .userbenchmark; fi
          MANUAL_WORKFLOW="${{ github.event.inputs.userbenchmark_name }}"
          if [ -z "${MANUAL_WORKFLOW}" ]; then
            python run_benchmark.py "${{ github.event.inputs.userbenchmark_name }}"
            cp -r ./.userbenchmark/"${{ github.event.inputs.userbenchmark_name }}" ../benchmark-output
          else
            # Figure out what userbenchmarks we should run today
            SHOULD_RUN_WORKLOADS=`python ./.github/scripts/userbenchmarks/get-should-run-benchmarks.py --runner a100`
            # run each of them
            # copy the result directory
            cp -r ./.userbenchmark ../benchmark-output
          fi
      - name: Upload result jsons to Scribe
        run: |
          . "${SETUP_SCRIPT}" && conda activate "${CONDA_ENV_NAME}"
          pushd benchmark
          RESULTS=$(find ${PWD}/../benchmark-output -name "*.json" -maxdepth 2 | sort -r)
          echo "Uploading results: ${RESULTS}"
          for r in ${RESULTS}; do
            python ./scripts/userbenchmark/upload_scribe.py --userbenchmark_json ${LATEST_RESULT}
          done
      - name: Upload artifact
        uses: actions/upload-artifact@v2
        with:
          name: TorchBench result
          path: benchmark-output/
      - name: Remove conda environment
        run: |
          conda env remove --name "${CONDA_ENV_NAME}"
