import random

import numpy as np
import torch

from bert_pytorch import parse_args
from torch.utils.data import DataLoader

from .dataset import BERTDataset, WordVocab
from .model import BERT
from .trainer import BERTTrainer


def train():
    # Make all randomness deterministic
    random.seed(1337)
    torch.manual_seed(1337)
    np.random.seed(1337)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False

    args = parse_args()

    print("Loading Vocab", args.vocab_path)
    vocab = WordVocab.load_vocab(args.vocab_path)
    print("Vocab Size: ", len(vocab))

    print("Loading Train Dataset", args.train_dataset)
    train_dataset = BERTDataset(
        args.train_dataset,
        vocab,
        seq_len=args.seq_len,
        corpus_lines=args.corpus_lines,
        on_memory=args.on_memory,
    )

    print("Loading Test Dataset", args.test_dataset)
    test_dataset = (
        BERTDataset(
            args.test_dataset, vocab, seq_len=args.seq_len, on_memory=args.on_memory
        )
        if args.test_dataset is not None
        else None
    )

    print("Creating Dataloader")
    train_data_loader = DataLoader(
        train_dataset, batch_size=args.batch_size, num_workers=args.num_workers
    )
    test_data_loader = (
        DataLoader(
            test_dataset, batch_size=args.batch_size, num_workers=args.num_workers
        )
        if test_dataset is not None
        else None
    )

    print("Building BERT model")
    bert = BERT(
        len(vocab), hidden=args.hidden, n_layers=args.layers, attn_heads=args.attn_heads
    )

    if args.script:
        print("Scripting BERT model")
        bert = torch.jit.script(bert)

    print("Creating BERT Trainer")
    trainer = BERTTrainer(
        bert,
        len(vocab),
        train_dataloader=train_data_loader,
        test_dataloader=test_data_loader,
        lr=args.lr,
        betas=(args.adam_beta1, args.adam_beta2),
        weight_decay=args.adam_weight_decay,
        device=args.device,
        device_ids=args.device_ids,
        log_freq=args.log_freq,
        debug=args.debug,
    )

    print("Training Start")
    for epoch in range(args.epochs):
        trainer.train(epoch)
        trainer.save(epoch, args.output_path)

        if test_data_loader is not None:
            trainer.test(epoch)
